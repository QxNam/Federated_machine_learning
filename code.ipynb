{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"sg0_WU16yiNT"},"source":["# Content table\n","\n","1. Client\n","2. Server"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6vtHwNt62CVi"},"source":["## Installing dependencies"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684749103780,"user":{"displayName":"Nam Quách","userId":"06419406297716564849"},"user_tz":-420},"id":"Icnuta7uxtaX","outputId":"6353fe9e-d6e0-4594-8a36-763ad79d2c68"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m\u001b[92mDay:\u001b[0m 2023-05-26 20:20:24.984168\n","\u001b[1m\u001b[92mDevice:\u001b[0m cuda\n","\u001b[1m\u001b[92mCore:\u001b[0m 8\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import cv2\n","import datetime\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, random_split\n","\n","import flwr as fl\n","from torchsummary import summary\n","from collections import OrderedDict\n","from typing import Dict, List, Optional, Tuple\n","\n","import color\n","import os\n","import multiprocessing as mp\n","import warnings\n","warnings.filterwarnings('ignore')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cores = mp.cpu_count()\n","c = color.clr()\n","print(c.SUCCESS('Day:'), datetime.datetime.now())\n","print(c.SUCCESS('Device:'), device)\n","print(c.SUCCESS('Core:'), cores)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CHpTh7232JX6"},"source":["# 1. Setup client"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15973,"status":"ok","timestamp":1684748713085,"user":{"displayName":"Nam Quách","userId":"06419406297716564849"},"user_tz":-420},"id":"Hf1it3i1zDBm","outputId":"3e87f49d-abf9-4ff2-d833-30c8104d27e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m\u001b[92mEpoch 1/2\u001b[0m\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/17 [00:03<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 310\u001b[0m\n\u001b[0;32m    308\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m    309\u001b[0m client1 \u001b[39m=\u001b[39m client(\u001b[39m'\u001b[39m\u001b[39mMIAS\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m16\u001b[39m, model, criteria, optimizer)\n\u001b[1;32m--> 310\u001b[0m client1\u001b[39m.\u001b[39;49mrun(\u001b[39m2\u001b[39;49m)\n","Cell \u001b[1;32mIn[8], line 283\u001b[0m, in \u001b[0;36mclient.run\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m--> 283\u001b[0m     lst_acc, lst_loss \u001b[39m=\u001b[39m fit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, device, num_epochs\u001b[39m=\u001b[39;49mepochs)\n\u001b[0;32m    284\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m    285\u001b[0m     plt\u001b[39m.\u001b[39msuptitle(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mClient \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_client\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n","Cell \u001b[1;32mIn[8], line 254\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mprint\u001b[39m(c\u001b[39m.\u001b[39mSUCCESS(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, num_epochs)))\n\u001b[0;32m    252\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m    255\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    256\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m - Train Acc: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(train_loss, train_acc))\n","Cell \u001b[1;32mIn[8], line 206\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    204\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    205\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 206\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m    207\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    208\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32md:\\semeter\\Machine_leaning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[8], line 145\u001b[0m, in \u001b[0;36mGoogLeNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 145\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    146\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool1(x)\n\u001b[0;32m    147\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n","File \u001b[1;32md:\\semeter\\Machine_leaning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\semeter\\Machine_leaning\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32md:\\semeter\\Machine_leaning\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","BATCH_SIZE = 16\n","SIZE_IMAGE = (227, 227)\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5), (0.5)),\n","                                transforms.RandomHorizontalFlip(),\n","                                transforms.RandomVerticalFlip(),\n","                                transforms.Resize(SIZE_IMAGE)])\n","\n","def pre_process(img):\n","    '''\n","    pre-process image before training\n","    algorithm: CLAHE (Contrast Limited Adaptive Histogram Equalization)\n","    '''\n","    img = img.astype(np.uint8)\n","    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n","    img_clahe = clahe.apply(img)\n","    return img_clahe\n","\n","# --- Data ---\n","class Dataset():\n","    def __init__(self, path_data, transform=None):\n","        self.path_data = path_data.upper()\n","        self.df = pd.read_csv(f'dataset/{self.path_data}-ROI-Mammography/description.csv')\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        image_path = f'dataset/{self.path_data}-ROI-Mammography/{self.df.iloc[index].Path_save}'\n","        image = cv2.imread(image_path, 0)\n","        if self.transform:\n","            image = pre_process(image)\n","            image = self.transform(image)\n","        label = self.df.iloc[index]['Cancer']\n","        return image, label\n","    \n","def train_test_split(dataset, test_size=0.2):\n","    s_test = int(test_size * dataset.__len__())\n","    s_train = dataset.__len__() - s_test\n","    train_dataset, test_dataset = random_split(dataset, [s_train, s_test])\n","    return train_dataset, test_dataset\n","    \n","def dataloader(train_dataset, test_dataset, batch_size=16):\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","    return train_dataloader, test_dataloader\n","\n","def show_transform(dataloader):\n","    images, labels = next(iter(dataloader))\n","    # Reshape and convert images to a NumPy array\n","    # matplotlib requires images with the shape (height, width, 3)\n","    images = images.permute(0, 2, 3, 1).numpy()\n","    # Denormalize\n","    images = images / 2 + 0.5\n","    # Create a figure and a grid of subplots\n","    fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n","    # Loop over the images and plot them\n","    for i, ax in enumerate(axs.flat):\n","        ax.imshow(images[i], cmap='gray')\n","        ax.set_title(labels[i].item())\n","        ax.axis(\"off\")\n","    # Show the plot\n","    fig.tight_layout()\n","    plt.show()\n","\n","def show_origin(dataset):\n","    fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n","    for i, ax in enumerate(axs.flat):\n","        img = cv2.imread(f'dataset/{dataset.path_data}-ROI-Mammography/{dataset.df.iloc[i].Path_save}', 0)\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(dataset.df.iloc[i].Cancer)\n","        ax.axis(\"off\")\n","    fig.tight_layout()\n","    plt.show()\n","    \n","# --- Model ---\n","# google net\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, out1x1, reduce3x3, out3x3, reduce5x5, out5x5, out1x1pool):\n","        super(InceptionModule, self).__init__()\n","\n","        # 1x1 convolution branch\n","        self.branch1x1 = nn.Conv2d(in_channels, out1x1, kernel_size=1)\n","\n","        # 1x1 convolution followed by 3x3 convolution branch\n","        self.branch3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, reduce3x3, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(reduce3x3, out3x3, kernel_size=3, padding=1)\n","        )\n","\n","        # 1x1 convolution followed by 5x5 convolution branch\n","        self.branch5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, reduce5x5, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(reduce5x5, out5x5, kernel_size=5, padding=2)\n","        )\n","\n","        # 3x3 max pooling followed by 1x1 convolution branch\n","        self.branch1x1pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, out1x1pool, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","        branch3x3 = self.branch3x3(x)\n","        branch5x5 = self.branch5x5(x)\n","        branch1x1pool = self.branch1x1pool(x)\n","        outputs = [branch1x1, branch3x3, branch5x5, branch1x1pool]\n","        return torch.cat(outputs, 1)\n","\n","class GoogLeNet(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super(GoogLeNet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n","        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n","        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n","\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n","        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n","        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n","        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n","        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n","\n","        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n","        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.4)\n","        self.fc = nn.Linear(1024, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.maxpool2(x)\n","        x = self.inception3a(x)\n","        x = self.inception3b(x)\n","        x = self.maxpool3(x)\n","        x = self.inception4a(x)\n","        x = self.inception4b(x)\n","        x = self.inception4c(x)\n","        x = self.inception4d(x)\n","        x = self.inception4e(x)\n","        x = self.maxpool4(x)\n","        x = self.inception5a(x)\n","        x = self.inception5b(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n","\n","# lenet\n","class Net(nn.Module):\n","    def __init__(self) -> None:\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 53 * 53, 1280)\n","        self.fc2 = nn.Linear(1280, 128)\n","        self.fc3 = nn.Linear(128, 2)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 53 * 53)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def get_parameters(net) -> List[np.ndarray]:\n","    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n","\n","def set_parameters(net, parameters: List[np.ndarray]):\n","    params_dict = zip(net.state_dict().keys(), parameters)\n","    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n","    net.load_state_dict(state_dict, strict=True)\n","\n","# --- Training ---\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    loop = tqdm(train_loader, desc='Training')\n","    for inputs, labels in loop:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += torch.sum(predicted == labels.data).item()\n","        acc_cur = torch.sum(predicted == labels.data).item()/labels.size(0)\n","        loop.set_postfix(loss=loss.item(), acc=acc_cur)\n","    \n","    train_loss = running_loss / len(train_loader)\n","    train_acc = correct / total\n","    \n","    return train_loss, train_acc\n","\n","def validate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        loop = tqdm(val_loader, desc='Validation')\n","        for inputs, labels in loop:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += torch.sum(predicted == labels.data).item()\n","            acc_cur = torch.sum(predicted == labels.data).item()/labels.size(0)\n","            loop.set_postfix(loss=loss.item(), acc=acc_cur)\n","    \n","    val_loss = running_loss / len(val_loader)\n","    val_acc = correct / total\n","    \n","    return val_loss, val_acc\n","\n","def fit(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20):\n","    lst_acc = {'train': [], 'val': []}\n","    lst_loss = {'train': [], 'val': []}\n","    best_val_acc = 0.0\n","    for epoch in range(num_epochs):\n","        print(c.SUCCESS('Epoch {}/{}'.format(epoch+1, num_epochs)))\n","        print('-'*50)\n","\n","        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n","        val_loss, val_acc = validate(model, val_loader, criterion, device)\n","        print('Train Loss: {:.4f} - Train Acc: {:.4f}'.format(train_loss, train_acc))\n","        print('Val Loss: {:.4f} - Val Acc: {:.4f}'.format(val_loss, val_acc))\n","        print('+'*100)\n","\n","        lst_acc['train'].append(train_acc)\n","        lst_acc['val'].append(val_acc)\n","        lst_loss['train'].append(train_loss)\n","        lst_loss['val'].append(val_loss)\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'result/best_model.pth')\n","    print(c.SUCCESS('Training and validation completed!'))\n","    return lst_acc, lst_loss\n","\n","# --- client ---\n","class client():\n","    def __init__(self, name_client, batch_size, model, criterion, optimizer):\n","        self.name_client = name_client.upper()\n","        self.df = Dataset(self.name_client, transform)\n","        self.df_train, self.df_val = train_test_split(self.df)\n","        self.train_loader, self.val_loader = dataloader(self.df_train, self.df_val, batch_size)\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","\n","    def run(self, epochs=5):\n","        lst_acc, lst_loss = fit(self.model, self.train_loader, self.val_loader, self.criterion, self.optimizer, device, num_epochs=epochs)\n","        plt.figure(figsize=(15, 5))\n","        plt.suptitle(f'Client {self.name_client}')\n","        plt.subplot(1, 2, 1)\n","        plt.plot(lst_acc['train'], label='train')\n","        plt.plot(lst_acc['val'], label='val')\n","        plt.title('Accuracy')\n","        plt.legend()\n","        plt.subplot(1, 2, 2)\n","        plt.plot(lst_loss['train'], label='train')\n","        plt.plot(lst_loss['val'], label='val')\n","        plt.title('Loss')\n","        plt.legend()\n","        plt.savefig(f'result/{self.name_client}.png')\n","        plt.show()\n","    \n","    def get_summary(self):\n","        return summary(self.model, (1, 227, 227))\n","    \n","    def get_parameters(self):\n","        return get_parameters(self.model)\n","\n","model = GoogLeNet(num_classes=2).to(device)\n","summary(model, (1, 227, 227))\n","# model = Net().to(device)\n","# criteria = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(model.parameters(), lr=0.01)\n","# client1 = client('MIAS', 16, model, criteria, optimizer)\n","# client1.run(2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'function' object is not iterable","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m show_transform(dataloader)\n","Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mshow_transform\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_transform\u001b[39m(dataloader):\n\u001b[1;32m---> 50\u001b[0m     images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39;49m(dataloader))\n\u001b[0;32m     51\u001b[0m     \u001b[39m# Reshape and convert images to a NumPy array\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[39m# matplotlib requires images with the shape (height, width, 3)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n","\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"]}],"source":["show_transform(dataloader)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMo4iTCT/x6nVpttW1ME/L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
